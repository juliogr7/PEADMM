# Learned Latent Space Initialization for ADMM with Generative Priors

Computational Imaging (CI) enables the acquisition of low-dimensional or corrupted observations that can be recovered as high-quality images by solving an inverse problem using computational algorithms. The Alternating Direction Method of Multipliers (ADMM) is widely used due to its robustness in handling constraints and regularization techniques. Recent advances in generative models have established them as powerful priors that can be integrated into ADMM to improve image recovery in challenging ill-posed inverse problems. However, ADMM with generative priors typically starts the recovery process by sampling the latent space from a standard normal distribution, particularly in highly underdetermined settings. This random initialization causes the recovery to begin at arbitrary points in the output space of the generator, hindering convergence and reducing the likelihood of reaching optimal solutions. Therefore, a two-stage approach is proposed for learning the latent space of the generative model to improve recovery performance. In the first stage, the latent space is estimated from the observations using an encoder model that leverages a pre-trained generative model. In the second stage, the learned latent space is used to initialize the ADMM algorithm with the generator model serving as the prior. Experimental results show that the proposed method improves PSNR by approximately 2 dB in the most challenging cases, outperforming the ADMM algorithm with standard normal latent space initialization.
